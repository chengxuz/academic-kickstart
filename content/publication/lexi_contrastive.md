+++
title = "Lexicon-Level Contrastive Visual-Grounding Improves Language Modeling"
date = 2024-02-15T21:17:06-07:00
draft = false

# Authors. Comma separated list, e.g. `["Bob Smith", "David Jones"]`.
authors = ["Chengxu Zhuang", "Evelina Fedorenko", "Jacob Andreas"]

# Publication type.
# Legend:
# 0 = Uncategorized
# 1 = Conference paper
# 2 = Journal article
# 3 = Manuscript
# 4 = Report
# 5 = Book
# 6 = Book section
publication_types = ["1"]

# Publication name and optional abbreviated version.
publication = "arXiv"
publication_short = ""

# Abstract and optional shortened version.
abstract = "Today’s most accurate language models aretrained on orders of magnitude more language data than human language learners receive—but no supervision from other sensory modalitiesthat play a crucial role in human learning. Can we make LMs’ representations and predictions more accurate (and more humanlike) with more ecologically plausible supervision? This paper describes Lexi-Contrastive Grounding, a grounded language learning procedure that leverages visual supervision to improve textual representations. Lexi-Contrastive Grounding combines a next-token prediction strategy with a contrastive visual-grounding objective, focusing on early-layer representations that encode lexical information. Across multiple word-learning and sentence-understanding benchmarks, Lexi-Contrastive Grounding not only outperforms traditionallanguage-only models in terms of learning efficiency, but also improves upon vision-andlanguagelearning procedures including CLIP, GIT, and Flamingo. Moreover, LexiContrastiveGrounding improves perplexity by around 5%on multiple language modeling tasks. Thiswork underscores the potential of incorporatingvisual grounding into language models, aligningmore closely with the multimodal nature ofhuman language acquisition."
abstract_short = ""

# Featured image thumbnail (optional)
image_preview = ""

# Is this a selected publication? (true/false)
selected = false

# Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's filename without extension.
#   E.g. `projects = ["deep-learning"]` references `content/project/deep-learning.md`.
#   Otherwise, set `projects = []`.
projects = []

# Tags (optional).
#   Set `tags = []` for no tags, or use the form `tags = ["A Tag", "Another Tag"]` for one or more tags.
tags = []

# Links (optional).
url_pdf = ""
url_preprint = ""
url_code = ""
url_dataset = ""
url_project = ""
url_slides = ""
url_video = ""
url_poster = ""
url_source = ""

# Custom links (optional).
#   Uncomment line below to enable. For multiple links, use the form `[{...}, {...}, {...}]`.
# url_custom = [{name = "Custom Link", url = "http://example.org"}]

# Does this page contain LaTeX math? (true/false)
math = true 

# Does this page require source code highlighting? (true/false)
highlight = true

# Featured image
# Place your image in the `static/img/` folder and reference its filename below, e.g. `image = "example.jpg"`.
[header]
image = ""
caption = ""

+++
